---
clusterName: ${clusterName}

# renovate: datasource=github-releases depName=siderolabs/talos
talosVersion: v1.11.6

# renovate: datasource=github-releases depName=siderolabs/kubelet
kubernetesVersion: v1.34.1

endpoint: https://talos-master-0:6443

allowSchedulingOnMasters: true
allowSchedulingOnControlPlanes: true

additionalApiServerCertSans:
- talos-master-0.tailnet-4d89.ts.net
- talos-master-1.tailnet-4d89.ts.net
- talos-master-2.tailnet-4d89.ts.net
- bastion-k8s.tailnet-4d89.ts.net
- 100.120.223.43

additionalMachineCertSans: []

clusterPodNets:
  - 10.244.0.0/16
clusterSvcNets:
  - 10.245.0.0/16

cniConfig:
  name: none

# yamllint disable
.schematics:
  basic-vm:
    id-labels: &schematicBasicVMIDLabels
      factory.talos.dev/schematic-id.part-0: 077514df2c1b6436460bc60faabc9766
      factory.talos.dev/schematic-id.part-1: 87b16193b8a1290fda4366c69024fec2
    config: &schematicBasicVM
      customization:
        systemExtensions:
            officialExtensions:
                - siderolabs/iscsi-tools
                - siderolabs/qemu-guest-agent
                - siderolabs/tailscale
                - siderolabs/util-linux-tools
  nvidia-gpu-vm:
    id-labels: &schematicNvidiaGPUVMIDLabels
      factory.talos.dev/schematic-id.part-0: 83991e32a1680c85e556346bacabc925
      factory.talos.dev/schematic-id.part-1: daea93d93a43169977c9eb930bb73d75
    config: &schematicNvidiaGPUVM
      customization:
        systemExtensions:
          officialExtensions:
            - siderolabs/amd-ucode
            - siderolabs/amdgpu  # renamed from amdgpu-firmware in 1.9
            - siderolabs/binfmt-misc
            - siderolabs/fuse3
            - siderolabs/i915  # renamed from i915-ucode in 1.9
            - siderolabs/intel-ice-firmware
            - siderolabs/intel-ucode
            - siderolabs/iscsi-tools
            - siderolabs/nonfree-kmod-nvidia-production
            - siderolabs/nvidia-container-toolkit-production
            - siderolabs/qemu-guest-agent
            - siderolabs/qlogic-firmware
            - siderolabs/realtek-firmware
            - siderolabs/tailscale
            - siderolabs/uinput
            - siderolabs/util-linux-tools

nodes:
## VMs
  - hostname: talos-master-0
    ipAddress: talos-master-0
    controlPlane: true
    installDiskSelector:
      size: ">=250GB"
    disableSearchDomain: false
    networkInterfaces:
      - deviceSelector:
          hardwareAddr: bc:24:11:d4:fa:d1
        dhcp: true
    schematic: *schematicBasicVM
    nodeLabels:
      <<: *schematicBasicVMIDLabels
      topology.kubernetes.io/region: home
      topology.kubernetes.io/zone: home-office-rack
      node.longhorn.io/create-default-disk: "true"
  - hostname: talos-master-1
    ipAddress: talos-master-1
    controlPlane: true
    installDiskSelector:
      size: ">=250GB"
    disableSearchDomain: false
    networkInterfaces:
      - deviceSelector:
          hardwareAddr: bc:24:11:2a:f8:5c
        dhcp: true
    schematic: *schematicBasicVM
    nodeLabels:
      <<: *schematicBasicVMIDLabels
      topology.kubernetes.io/region: home
      topology.kubernetes.io/zone: home-office-rack
      node.longhorn.io/create-default-disk: "true"
  - hostname: talos-master-2
    ipAddress: talos-master-2
    controlPlane: true
    installDiskSelector:
      size: ">=250GB"
    disableSearchDomain: false
    networkInterfaces:
      - deviceSelector:
          hardwareAddr: bc:24:11:4a:1e:8e
        dhcp: true
    schematic: *schematicBasicVM
    nodeLabels:
      <<: *schematicBasicVMIDLabels
      topology.kubernetes.io/region: home
      topology.kubernetes.io/zone: home-office-rack
      node.longhorn.io/create-default-disk: "true"
  - hostname: talos-worker-gpu-0
    ipAddress: talos-worker-gpu-0
    controlPlane: false
    installDiskSelector:
      size: ">=32GB"
    disableSearchDomain: false
    schematic: *schematicNvidiaGPUVM
    nodeLabels:
      <<: *schematicNvidiaGPUVMIDLabels
      topology.kubernetes.io/region: home
      topology.kubernetes.io/zone: home-office-rack
      node.longhorn.io/create-default-disk: "false"
    patches:
      - "@./patch-nvidia-gpu.yaml"
  - hostname: talos-worker-gpu-1
    ipAddress: talos-worker-gpu-1
    controlPlane: false
    installDiskSelector:
      size: ">=32GB"
    disableSearchDomain: false
    schematic: *schematicNvidiaGPUVM
    nodeLabels:
      <<: *schematicNvidiaGPUVMIDLabels
      topology.kubernetes.io/region: home
      topology.kubernetes.io/zone: home-office-rack
      node.longhorn.io/create-default-disk: "false"
    extensionServices:
      - name: tailscale
        environment:
          - TS_AUTH_KEY=${TAILSCALE_AUTHKEY}
          - TS_ACCEPT_DNS=true
          - TS_EXTRA_ARGS=--advertise-connector --advertise-tags=tag:k8s-cluster-bastion
    patches:
      - "@./patch-nvidia-gpu.yaml"
# ## SBC'S ##
#   - hostname: raspberrypi-dca632210df0.tailnet-4d89.ts.net
#     ipAddress: raspberrypi-dca632210df0.tailnet-4d89.ts.net
#     controlPlane: false
#     installDisk: /dev/mmcblk0
#     disableSearchDomain: false
#     networkInterfaces:
#       - interface: eno1
#         mtu: 0
#         dhcp: true
#     nodeLabels:
#       topology.kubernetes.io/region: home
#       topology.kubernetes.io/zone: home-sbc
#   - hostname: raspberrypi-dca632210a61.tailnet-4d89.ts.net
#     ipAddress: raspberrypi-dca632210a61.tailnet-4d89.ts.net
#     controlPlane: false
#     installDisk: /dev/mmcblk0
#     disableSearchDomain: false
#     networkInterfaces:
#       - interface: eno1
#         mtu: 0
#         dhcp: true
#     nodeLabels:
#       topology.kubernetes.io/region: home
#       topology.kubernetes.io/zone: home-sbc
#   - hostname: raspberrypi-dca632210f4e.tailnet-4d89.ts.net
#     ipAddress: raspberrypi-dca632210f4e.tailnet-4d89.ts.net
#     controlPlane: false
#     installDisk: /dev/mmcblk0
#     disableSearchDomain: false
#     networkInterfaces:
#       - interface: eno1
#         mtu: 0
#         dhcp: true
#     nodeLabels:
#       topology.kubernetes.io/region: home
#       topology.kubernetes.io/zone: home-sbc
patches:
- |
  machine:
    files:
      - content: |
          [plugins."io.containerd.grpc.v1.cri"]
            enable_unprivileged_ports = true
            enable_unprivileged_icmp = true
        op: create
        path: /etc/cri/conf.d/20-customization.part
    time:
      disabled: false
      servers:
        - time.cloudflare.com
    kubelet:
      extraArgs:
        feature-gates: ImageVolume=true
      nodeIP:
        validSubnets:
          - 192.168.20.0/24
    features:
      kubePrism:
        enabled: true
        port: 7445
      hostDNS:
        enabled: true
        forwardKubeDNSToHost: true
        resolveMemberNames: true
    network:
      nameservers:
        - 100.100.100.100
        - 76.76.2.22
        - 2606:1a40::22
- '@./patch-longhorn.yaml'
- '@./patch-global-sysctls.yaml'
worker:
  extensionServices: &commonExtensionServices
    - name: tailscale
      environment:
        - TS_AUTH_KEY=${TAILSCALE_AUTHKEY}
        - TS_ACCEPT_DNS=true
        - TS_EXTRA_ARGS=--accept-routes --advertise-tags=tag:k8s-cluster-bastion
controlPlane:
  extensionServices:
    - name: tailscale
      environment:
        - TS_AUTH_KEY=${TAILSCALE_AUTHKEY}
        - TS_ACCEPT_DNS=true
        - TS_EXTRA_ARGS=--advertise-connector --advertise-tags=tag:k8s-cluster-bastion
        - TS_SERVE_CONFIG=/etc/tailscale/serve.json
      configFiles:
        - mountPath: /etc/tailscale/serve.json
          content: |
            {
              "Services": {
                "svc:bastion-k8s": {
                  "TCP": {
                    "6443": {
                      "TCPForward": "127.0.0.1:6443"
                    }
                  }
                }
              }
            }
  patches:
    - |
      machine:
        pods:
          - apiVersion: v1
            kind: Pod
            metadata:
              name: ts-serve-advertise
              namespace: kube-system
            spec:
              hostNetwork: true
              restartPolicy: Always
              containers:
                - name: healthcheck
                  image: nicolaka/netshoot:latest
                  env:
                    - name: PROBE_INTERVAL
                      value: "10"
                    - name: PROBE_TIMEOUT
                      value: "5"
                    - name: FAILURE_THRESHOLD
                      value: "3"
                    - name: SUCCESS_THRESHOLD
                      value: "1"
                  command:
                    - /bin/sh
                    - -c
                    - |
                      PROBE_INTERVAL="$${PROBE_INTERVAL:-10}"
                      PROBE_TIMEOUT="$${PROBE_TIMEOUT:-5}"
                      FAILURE_THRESHOLD="$${FAILURE_THRESHOLD:-3}"
                      SUCCESS_THRESHOLD="$${SUCCESS_THRESHOLD:-1}"

                      consecutive_failures=0
                      consecutive_successes=0
                      is_advertised=false

                      echo "Starting health check loop..."
                      echo "  PROBE_INTERVAL: $${PROBE_INTERVAL}s"
                      echo "  PROBE_TIMEOUT: $${PROBE_TIMEOUT}s"
                      echo "  FAILURE_THRESHOLD: $${FAILURE_THRESHOLD}"
                      echo "  SUCCESS_THRESHOLD: $${SUCCESS_THRESHOLD}"

                      while true; do
                        if timeout "$${PROBE_TIMEOUT}" curl -sk https://127.0.0.1:6443/healthz > /dev/null 2>&1; then
                          consecutive_failures=0
                          consecutive_successes=$$((consecutive_successes + 1))
                          echo "[OK] Health check passed (successes: $${consecutive_successes}, advertised: $${is_advertised})"

                          if [ "$$is_advertised" = "false" ] && [ "$$consecutive_successes" -ge "$$SUCCESS_THRESHOLD" ]; then
                            echo "Threshold reached, advertising service..."
                            /tailscale/usr/local/bin/tailscale serve advertise svc:bastion-k8s
                            is_advertised=true
                          fi
                        else
                          consecutive_successes=0
                          consecutive_failures=$$((consecutive_failures + 1))
                          echo "[FAIL] Health check failed (failures: $${consecutive_failures}/$${FAILURE_THRESHOLD}, advertised: $${is_advertised})"

                          if [ "$$is_advertised" = "true" ] && [ "$$consecutive_failures" -ge "$$FAILURE_THRESHOLD" ]; then
                            echo "Threshold reached, draining service..."
                            /tailscale/usr/local/bin/tailscale serve drain svc:bastion-k8s
                            is_advertised=false
                          fi
                        fi

                        sleep "$${PROBE_INTERVAL}"
                      done
                  volumeMounts:
                    - name: tailscaled-sock
                      mountPath: /var/run/tailscale/tailscaled.sock
                    - name: tailscale-bin
                      mountPath: /tailscale
                  resources:
                    limits:
                      cpu: 50m
                      memory: 128Mi
                    requests:
                      cpu: 10m
                      memory: 64Mi
              volumes:
                - name: tailscaled-sock
                  hostPath:
                    path: /var/run/tailscale/tailscaled.sock
                - name: tailscale-bin
                  image:
                    reference: ghcr.io/tailscale/tailscale:v1.90.9
                    pullPolicy: IfNotPresent
    - |
      machine:
        certSANs:
          - 127.0.0.1
        features:
          kubernetesTalosAPIAccess:
            enabled: true
            allowedRoles:
              - os:admin
            allowedKubernetesNamespaces:
              - system-upgrade
    - |
      cluster:
        allowSchedulingOnMasters: true
        apiServer:
          admissionControl: null
          extraArgs:
            feature-gates: SELinuxMountReadWriteOncePod=false,ImageVolume=true # prevents .spec.seLinuxMount from being set on CSIDriver to work around https://github.com/argoproj/argo-cd/issues/13585
            bind-address: 0.0.0.0
            oidc-issuer-url: https://vault.jtcressy.net/v1/identity/oidc/provider/default
            oidc-client-id: Xg96CJ6FgSZzyoPlumEdhXBncKcNigwS
            oidc-groups-claim: groups
            oidc-groups-prefix: "vault:"
            oidc-username-claim: username
            oidc-username-prefix: "vault:"
          certSANs:
            - 127.0.0.1
        controllerManager:
          extraArgs:
            bind-address: 0.0.0.0
        discovery:
          enabled: true
          registries:
            kubernetes:
              disabled: true
            service: {}
        etcd:
          extraArgs:
            listen-metrics-urls: http://0.0.0.0:2381
          advertisedSubnets:
            - 192.168.20.0/24 # using underlay network for node-to-node communication
        proxy:
          disabled: true
        scheduler:
          extraArgs:
            bind-address: 0.0.0.0
